{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Computational Social Science**\n",
    "\n",
    "*Assignment 1*\n",
    "\n",
    "Link to github: https://github.com/Dribo/02467_A_1\n",
    "\n",
    "*Contribution statement*\n",
    "\n",
    "All the exercises have been completed in cooperation, where we have worked to complete one assignment before moving on to the next."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part 1: Using web-scraping to gather data**\n",
    "The following cells contain the code for solve part 1 of the exercise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\02467 - css\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host '2019.ic2s2.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'\n",
    "try:\n",
    "    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST += 'HIGH:!DH:!aNULL'\n",
    "except AttributeError:\n",
    "    # no pyopenssl support used / needed / available\n",
    "    pass\n",
    "\n",
    "# Getting the web-page\n",
    "link19oral = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "link19post = \"https://2019.ic2s2.org/posters/\"\n",
    "r = requests.get(link19oral, verify=False)\n",
    "soup = BeautifulSoup(r.content)\n",
    "div = soup.find(\"div\", {\"class\":\"col-md-8\"})\n",
    "ps = div.findAll('p')\n",
    "relevant = ps[3:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Creating regex for filtering out names on the web-page for oral presentations\n",
    "# regex1 is for names that have an initial in the middle, regex2 is for all other names\n",
    "regex1 = r\" ([a-zA-ZÀ-ÿ]* ([A-Z]. )+[a-zA-ZÀ-ÿ]+[.,])\"\n",
    "regex2 = r\" ([a-zA-ZÀ-ÿ]* [a-zA-ZÀ-ÿ]+[.,])\"\n",
    "names = []\n",
    "names1 = []\n",
    "# Here we loop through the relevant part of the web-page and use the RegEx to find all the names and add them to a list\n",
    "for r in relevant:\n",
    "    retrieved = re.findall(regex1, str(r), re.UNICODE)\n",
    "    names += retrieved\n",
    "\n",
    "# Remove trailing spaces/dots\n",
    "for i in range(len(names)):\n",
    "    names[i] = names[i][0][:-1]\n",
    "\n",
    "# This is the same but for regex2\n",
    "for r in relevant:\n",
    "    retrieved = re.findall(regex2, str(r), re.UNICODE)\n",
    "    names1 += retrieved\n",
    "\n",
    "for i in range(len(names1)):\n",
    "    names1[i] = names1[i][:-1]\n",
    "\n",
    "names += names1\n",
    "list_of_presenters = names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\02467 - css\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host '2019.ic2s2.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Getting the web-page for the posters\n",
    "r = requests.get(link19post, verify=False)\n",
    "soup = BeautifulSoup(r.content)\n",
    "uls = soup.findAll('ul')\n",
    "r1 = list(uls)[6]\n",
    "r2 = list(uls)[7]\n",
    "\n",
    "relevant1 = []\n",
    "relevant2 = []\n",
    "\n",
    "# Splitting the strings at new-lines\n",
    "for x in list(r1):\n",
    "    if x != '\\n':\n",
    "        relevant1.append(x)\n",
    "for x in list(r2):\n",
    "    if x != '\\n':\n",
    "        relevant2.append(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of presenters with dublicates: 742\n",
      "List of presnters without dublicates : 613\n"
     ]
    }
   ],
   "source": [
    "# Regex for filtering of posters\n",
    "regex1 = r\"<li>[a-zA-Z ]*<span>\"\n",
    "names = []\n",
    "names1 = []\n",
    "names2 = []\n",
    "names3 = []\n",
    "# The same as before where we use the regex to filter the strings and find the names\n",
    "for r in relevant1:\n",
    "    retrieved = re.findall(regex1, str(r), re.UNICODE)\n",
    "    names += retrieved\n",
    "\n",
    "for i in range(len(names)):\n",
    "    names[i] = names[i][4:-6]\n",
    "    names[i] = names[i].split(' and ')\n",
    "    for j in range(len(names[i])):\n",
    "        names1 += [names[i].pop(0)]\n",
    "\n",
    "for r in relevant2:\n",
    "    retrieved = re.findall(regex1, str(r), re.UNICODE)\n",
    "    names2 += retrieved\n",
    "\n",
    "for i in range(len(names2)):\n",
    "    names2[i] = names2[i][4:-6]\n",
    "    names2[i] = names2[i].split(' and ')\n",
    "    for j in range(len(names2[i])):\n",
    "        names3 += [names2[i].pop(0)]\n",
    "\n",
    "list_of_presenters += names1 + names3\n",
    "# Create the list of presenters and turn it into a set to remove dublicates\n",
    "print('List of presenters with dublicates:', len(list_of_presenters))\n",
    "print('List of presnters without dublicates :',  len(set(list_of_presenters)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Part 1*\n",
    "\n",
    "    2. We got 613 unique authors in 2019 for both posters and oral presentations\n",
    "\n",
    "    3. One decision that was taken during the web-scraping was to use RegEx to filter the out everything but the names. What this did was result in some oddities, where some elements got included that weren't names, and some names were cut off a bit too early, especially if they contained an intial as a middle name."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}